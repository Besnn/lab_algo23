%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.2 (2020-10-22)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left
\usepackage{listings}
\usepackage[english]{babel} % Specify a different language here - english by default
%\usepackage[
%backend=biber,
%style=alphabetic,
%]{biblatex}
%\bibliographystyle{ref.bib}
\usepackage{lipsum} % Required to insert dummy text. To be removed otherwise

%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{200,0,40} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings
\definecolor{color3}{RGB}{40, 60, 100} % Color of links
%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks

\hypersetup{
	hidelinks,
	colorlinks,
	breaklinks=true,
	urlcolor=color3,
	citecolor=color1,
	linkcolor=color1,
	bookmarksopen=false,
	pdftitle={Title},
	pdfauthor={Author},
}
\newcommand{\link}[2]{
  \href{#2}{\bf[#1]}
}
%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\JournalInfo{Course Assignment Paper for the Distributed Systems Class} % Journal information
\Archive{at the University of Florence} % Additional notes (e.g. copyright, DOI, review/research article)

\PaperTitle{On Single Broker Partition Scalability in Apache Kafka with Zookeeper and Quorum Controller} % Article title

\Authors{Besnik Nuro} % Authors
%\affiliation{\textsuperscript{1}\textit{Department of Biology, University of Examples, London, United Kingdom}} % Author affiliation
%\affiliation{\textsuperscript{2}\textit{Department of Chemistry, University of Examples, London, United Kingdom}} % Author affiliation
%\affiliation{*\textbf{Corresponding author}: john@smith.com} % Corresponding author

\Keywords{Apache Kafka --- Apache Zookeeper --- KRaft --- Stressing Kafka Broker --- JVM --- Docker --- AWS} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{Apache Kafka has been described as highly scalable. The official Apache blog asserts that 200 thousand as a limit on the number of partitions, while Zookeeper-less configurations have a theoretical limit of 1 million partitions. With this paper, I put to test the limits of this scalability and explore possible remedies that can be applied. Note that these tests represent best cases and simplified configuration and deployment, and make assumptions that might not be true in a production environment.}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the title and abstract box

\tableofcontents % Output the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction} % The \section*{} command stops section numbering

\addcontentsline{toc}{section}{Introduction} % Adds this section to the table of contents
Apache Kafka is the leading technology in event streaming. As stated on their site, over 80\% of Fortune 100 companies use Kafka. New York Times uses Kafka to keep a permanent log of all events relating to their day-to-day operations \link{confluent-nytimes}{https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/}. Kafka is remarked as being extremely efficient. This is owed to its implementation in JVM, as well as a custom TCP protocol and many mechanisms for maximizing throughput (partitions and batch streaming among other subtleties). It is also robust and fault-tolerant, in that there is more than one node holding copies of event logs and the cluster can reorganize itself to account for the loss of its nodes (in combination with Zookeeper cluster). 

Despite it's efficiency, there are physical limits on its scalability: 200 thousand partitions spread across a cluster \link{apache-blog}{https://blogs.apache.org/kafka/entry/apache-kafka-supports-more-partitions} with Zookeeper, and allegedly a few millions for a KRaft cluster \link{confluent-kraft}{https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/\#scaling-up}.

For my tests, I chose to go with Amazon Web Services. They offer a free tier services which will be useful to the experiment, but the Kafka Broker will have to be paid for. To approximate a real life cluster, maximize server throughput and minimize costs, I will go over some facts and assumptions I used to decide which instance type to use.
%\footnote{And some mathematics $\cos\pi=-1$ and $\alpha$ in the text.}.

%------------------------------------------------

\section{Preliminary Details}

\begin{figure*}[ht]\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\includegraphics[width=\linewidth]{architecture}
	\caption{Producer-Broker Architecture}
	\label{fig:architecture}
\end{figure*}

In order to approximate a realistic deployment of Kafka, we need to take into account Kafka's, Zookeeper's and Quorum Controller's needs and mode of operation. Let's start by establishing some facts.

\subsection{Storage Layer}
Kafka's storage layer is engineered in such a way as to maximize throughput. We go over a few aspects of how data is managed and stored in order to understand the storage model. 
\subsubsection{Messages}
Messages (a message is the data representation of an event) are records containing 2 fields for the key-value pair and several other transparent ones for metadata. Namely \link{kafka-docs}{https://kafka.apache.org/documentation/\#record}:
\begin{lstlisting}
length: varint
attributes: int8
bit 0-7: unused
timestampDelta: varlong
offsetDelta: varint
keyLength: varint
key: byte[]
valueLen: varint
value: byte[]
Headers => [Header]
\end{lstlisting}
We won't be using record headers, so the implementation doesn't concern us.

\subsubsection{Batching}
In order to provide efficient reads, writes and high throughput, Kafka batches records together \link{kafka-docs}{https://kafka.apache.org/documentation/\#recordbatch}.
The size of the batches can be tweaked accordingly via the appropriate configuration settings.
Record batches are stored in-memory until the need arises to store them in the disk or other secondary memory. This makes for fast reads and reasonable write speeds depending on the medium. On hard media, the batching makes for less travel time of the magnetic head. On solid-state drives, the write speeds are reasonable anyhow, but the lower overhead makes for faster sequential speeds relative to random in SSDs.

\iffalse
\link{ddg-query}{https://duckduckgo.com/?q=why+are+sequential+writes+faster+in+ssds&t=ffab&ia=web}.
\fi

\subsubsection{Topics and Partitions}
Topics are a way of organizing events in Kafka. As the name suggests, events relate to a topic. Topics employ append-only record buckets called partitions. Which partition a record ends up in is determined by the hash function in use by the producers. The standard implementation uses a \emph{murmur2} \link{github-repo}{https://github.com/apache/kafka/blob/3.3/clients/src/main/java/org/apache/kafka/common/utils/Utils.java\#L472} hash:
$$
i(k, n) = murmur2(k)\mod \ \ \ n
$$
The function works much like a bloom filter: events with the same key will always  qend up in the same partition provided the number of partitions doesn't increase.

A high partition number can increase throughput via IO parallelism. However, that means more open file handles and therefore more memory consumption.
\subsubsection{Replication}
Kafka provides replicas as a way to have decentralized partition backups. Each replica-leader assigns $R - 1$ other nodes as replica-followers in which to store the backups,  $R$ being the replication factor. Too high a replication factor can add substantial latency to the cluster, too low poor reliability.
\subsection{System Requirements}
Broker server will need to be IO-optimized: fast and abundant RAM, a low-latency storage solution and high network bandwidth. These are the main bottlenecks, but other configuration settings can be adjusted for optimal operation during production.
\subsubsection{Hardware}
I chose to go with a \texttt{r6i.xlarge} instance. R6 instances are a family of memory-optimized servers and R6i instances offer better IO throughput compared to their R6 relatives. An extra-large instance of the R6i family offers 4 virtual processors at 3.5 GHz turbo speeds, 32 GiB of RAM, up to 10 Gb/s bandwidth to Elastic Block Storage partitions and up to 12.5 Gb/s of Network Bandwidth within the same service zone (North Virginia in our case). See fig. \ref{fig:architecture} for a rough schematic of the infrastructure.

An obvious bottleneck would be the storage configuration in use with our server. Ideally, Kafka and Zookeeper would be on separate servers and therefore use different volumes to read and write to. R6i instances support up to 10 Gbps EBS bandwidth provided the EBS volume in use is high-bandwidth solid state storage. While AWS offers IO-provisioned SSDs as EBS options, the costs are prohibitive and therefore I have to settle for separate general purpose SSD volumes for Kafka and Zookeeper.



\subsubsection{Underlying OS Configuration}
There are several OS aspects that will need to be tweaked in order for Kafka to scale well:
\begin{enumerate}
\item{Brokers write to the underlying filesystem, but this doesn't mean the OS flushes to disk on each change. In practice record batches are written in big sizes sequentially. A large disk cache size is therefore preferable;}
\item{Certain filesystems (those supporting journaling and IO-parallelism) complement Kafka. Kafka documentation recommends \texttt{xfs} or \texttt{ext4} \link{confluent-docs}{https://docs.confluent.io/platform/current/kafka/deployment.html\#filesystem}.
\texttt{xfs} in particular excels in parallel disk-IO use cases;}
\item{For each partition in memory there is an open file handle. Linux systems often limit these to 1024, but this setting can be changed to a much higher number. Windows Server offers generous limits on file descriptors and tweaking them in these regard doesn't need to be considered;}
\item{Swapping should be minimized. This is a bit of a problem because swap prevents processes from being killed when there is swap space available, and so is desirable in that regard. Documentation suggests setting swappiness on Linux to 1, the lowest possible value permitting swapping.}
\end{enumerate}

\subsubsection{JVM Considerations}
Zookeeper's JVM Heap should be kept small but reasonable, the reason being that swaps have to be avoided at all costs \link{zk-docs}{https://zookeeper.apache.org/doc/r3.8.0/zookeeperAdmin.html\#Single+Machine+Requirements}. A value between 2 and 4 GB should be sufficient.

Kafka brokers do not need much JVM heap allocated. Higher heap values are preferred as that limits garbage collection pauses and therefore uses less CPU time. 4 to 6 GB is usually enough. \link{cloudera-docs}{https://docs.cloudera.com/cdp-private-cloud-base/7.1.3/installation/topics/cdpdc-kafka.html}

\iffalse

Reference to figure \ref{fig:results}.

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{results}
	\caption{In-text Picture}
	\label{fig:results}
\end{figure}

\fi

%------------------------------------------------

\section{Setup}

\lipsum[10] % Dummy text

\subsection{Subsection}

\lipsum[11] % Dummy text

\begin{table}[hbt]
	\caption{Table of Grades}
	\centering
	\begin{tabular}{llr}
		\toprule
		\multicolumn{2}{c}{Name} \\
		\cmidrule(r){1-2}
		First name & Last Name & Grade \\
		\midrule
		John & Doe & $7.5$ \\
		Richard & Miles & $2$ \\
		\bottomrule
	\end{tabular}
	\label{tab:label}
\end{table}

\subsubsection{Subsubsection}

\lipsum[12] % Dummy text

\begin{description}
	\item[Word] Definition
	\item[Concept] Explanation
	\item[Idea] Text
\end{description}

\subsubsection{Subsubsection}

\lipsum[13] % Dummy text

\begin{itemize}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item First item in a list
	\item Second item in a list
	\item Third item in a list
\end{itemize}

\subsubsection{Subsubsection}

\lipsum[14] % Dummy text

\subsection{Subsection}

\lipsum[15-23] % Dummy text

%------------------------------------------------

\phantomsection
%\section*{References} % The \section*{} command stops section numbering

%\addcontentsline{toc}{section}{References} % Adds this section to the table of contents
%\printbibliography[title={References}]

%----------------------------------------------------------------------------------------

\end{document}